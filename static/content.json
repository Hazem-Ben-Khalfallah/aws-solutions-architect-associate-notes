[{"name":"Dashboard","path":"/","text":"","wordCount":0,"type":"TOC"},{"name":"Exam preparation path","description":"Exam preparation","topics":["preparation"],"path":"/exampreparation","text":"AWS Certification Exam Guide\nReview the exam guide, which contains the content outline and target audience for the certification exam. Perform a self-assessment to identify your knowledge or skills gaps.\n\nMy Preparation path\nGo through the ACloudGuru AWS certified solutions architect associate course [23.5 hrs]\nGo through the ACloudGuru AWS well architected framework () course [4.7 hrs*]\nGo through the ACloudGuru AWS RDS () course [3.1 hrs*]\nAttempt the practice tests by Neal Davis at Udemy. After each test, note down the concepts I had difficulties with.\nAttempt the practice tests by Jon Bonso at Udemy. Again, after each test, note down the concepts I had difficulties with.\n\nSo you should go through the notes only after you have done a course that explains the basics, such as the one from ACloudGuru.\n\n(*) This course is optionnal\n\nDeep dive\nThe following courses are by no means required to pass the AWS certification. They are listed to get a more in depth view about some AWS critical services.\nS3 master class course [10.7 hrs]\nIdentity and Access Management (IAM) Concepts course [1.5 hrs]\nHow to Properly Secure an S3 Bucket course [2.2 hrs]\nAWS Security Essentials course [11.2 hrs]\nLearn AWS by Doing course [19.5 hrs]\nBuilding a Full-Stack Serverless Application on AWS course [7.8 hrs]\nAmazon DynamoDB Deep Dive course [15.9 hrs]\n\nCore AWS Services to Focus On\nEC2 – As the most fundamental compute service offered by AWS, you should know about EC2 inside out.\nLambda – Lambda is the common service used for serverless applications. Study how it is integrated with other AWS services to build a full stack serverless app.\nELB – Load balancing is very important for a highly available system. Study about the different types of ELBs, and the features each of them supports.\nAuto Scaling – Study what services in AWS can be auto scaled, what triggers scaling, and how auto scaling increases/decreases the number of instances.\nEBS – As the primary storage solution of EC2, study on the types of EBS volumes available. Also study how to secure, backup and restore EBS volumes.\nS3 / Glacier – AWS offers many types of S3 storage depending on your needs. Study what these types are and what differs between them. Also review on the capabilities of S3 such as hosting a static website, securing access to objects using policies, lifecycle policies, etc. Learn as much about S3 as you can.\nStorage Gateway – There are occasional questions about Storage Gateway in the exam. You should understand when and which type of Storage Gateway should be used compared to using services like S3 or EBS. You should also know the use cases and differences between DataSync and Storage Gateway.\nEFS – EFS is a service highly associated with EC2, much like EBS. Understand when to use EFS, compared to using S3, EBS or instance store. Exam questions involving EFS usually ask the trade off between cost and efficiency of the service compared to other storage services.\nRDS / Aurora – Know how each RDS database differs from one another, and how they are different from Aurora. Determine what makes Aurora unique, and when it should be preferred from other databases (in terms of function, speed, cost, etc). Learn about parameter groups, option groups, and subnet groups.\nDynamoDB – The exam includes lots of DynamoDB questions, so read as much about this service as you can. Consider how DynamoDB compares to RDS, Elasticache and Redshift This service is also commonly used for serverless applications along with Lambda.\nElasticache – Familiarize yourself with Elasticache redis and its functions. Determine the areas/services where you can place a caching mechanism to improve data throughput, such as managing session state of an ELB, optimizing RDS instances, etc.\nVPC/NACL/Security Groups – Study every service that is used to create a VPC (subnets, route tables, internet gateways, nat gateways, VPN gateways, etc). Also, review on the differences of network access control lists and security groups, and during which situations they are applied.\nRoute 53 – Study the different types of records in Route 53. Study also the different routing policies. Know what hosted zones and domains are.\nCloudFront – Study how CloudFront helps speed up websites. Know what content sources CloudFront can serve from. Also check the kinds of certificates CloudFront accepts.\nIAM – Services such as IAM Users, Groups, Policies and Roles are the most important to learn. Study how IAM integrates with other services and how it secures your application through different policies. Also read on the best practices when using IAM.\nCloudWatch – Study how monitoring is done in AWS and what types of metrics are sent to CloudWatch. Also read upon Cloudwatch Logs, CloudWatch Alarms, and the custom metrics made available with CloudWatch Agent.\nCloudTrail – Familiarize yourself with how CloudTrail works, and what kinds of logs it stores as compared to CloudWatch Logs.\nKinesis – Read about Kinesis sharding and Kinesis Data Streams. Have a high level understanding of how each type of Kinesis Stream works.\nSQS – Gather info on why SQS is helpful in decoupling systems. Study how messages in the queues are being managed (standard queues, FIFO queues, dead letter queues). Know the differences between SQS, SNS, SES, and Amazon MQ.\nSNS – Study the function of SNS and what services can be integrated with it. Also be familiar with the supported recipients of SNS notifications.\nSWF / CloudFormation / OpsWorks – Study how these services function. Differentiate the capabilities and use cases of each of them. Have a high level understanding of the kinds of scenarios they are usually used in.\n","wordCount":897,"type":"Content"},{"name":"Well-Architected Framework","description":"Well-Architected Framework","topics":["Well-Architected Framework"],"path":"/wellarchitectedframework","text":"The five pillars are:\nOperational Excellence\nSecurity\nReliability\nPerformance Efficiency\nCost Optimization\n\nOperational Excellence\nDesign Principles\nPerform operations as code\nAnnotate documents\nMake frequent, small, reversible changes\nRefine operations procedures frequently\nAnticipate failure\nLearn from all operational failures\nBest Practices\nPrepare\nOperate\nEvolve\n\nKey AWS Service\nAWS CloudFormation\n\nSecurity\nDesign Principles\nImplement a strong identity foundations\nEnable traceability\nApply security at all layers\nAutomate security best practices\nProtect data in transit and at rest\nKeep people away from data\nPrepare for security events\n\nBest Practices\nIdentity and Access Management\nDetective Controls\nInfrastructure Protection\nData Protection\nIncident Response\n\nKey AWS Service\nAWS Identity and Access Management (IAM)\n\nReliability\nDesign Principles\nTest recovery procedures\nAutomatically recover from failure\nScale horizontally to increase aggregate system availability\nStop guessing capacity\nManage change in automation\n\nBest Practices\nFoundations\nChange Management\nFailure Management\n\nKey AWS Service\nAmazon CloudWatch\n\nPerformance Efficiency\nDesign Principles\nDemocratize advanced technologies\nGo global in minutes\nUse serverless architecture\nExperiment more often\nMechanical sympathy\n\nBest Practices\nSelection\n  Compute\n  Storage\n  Database\n  Network\nReview\nMonitoring\nTradeoffs\n\nKey AWS Service\nAmazon CloudWatch\n\nCost Optimization\nDesign Principles\nAdopt a consumption model\nMeasure overall efficiency\nStop spending money on data center operations\nAnalyze and attribute expenditure\nUse managed and application level services to reduce cost of ownership\n\nBest Practices\nExpenditure Awareness\nCost-Effective Resources\nMatching Supply and Demand\nOptimizing Over Time\n\nKey AWS Service\nCost Explorer\n","wordCount":149,"type":"Content"},{"name":"White Papers & FAQs","description":"Read White Papers & FAQs","topics":["white papers","FAQs"],"path":"/whitepapers","text":"Overview of Amazon Web Services: This paper provides a good introduction on Cloud Computing, the AWS Global Infrastructure, and the available AWS Services. Reading this whitepaper before proceeding to the other whitepapers below will clear up many jargons found on the succeeding materials.\nAWS Best Practices: This paper teaches you the best practices to perform when running your applications in AWS. It points out the advantages of Cloud over traditional hosting infrastructures and how you can implement them to keep your applications up and running all the time. The SA Associate exam will include questions that will test your knowledge on the best practices through different example scenarios.\nUsing Amazon Web Services for Disaster Recovery: This paper explains the different types of disaster recovery plans that you can perform in AWS. It is your responsibility as a Solutions Architect to mitigate any potential downtime when disaster strikes. Depending on your RPO and RTO, a proper disaster recovery plan will be a deciding factor between business continuity and revenue loss.\n\nAWS Well-Architected and the Five Pillars\nThis paper is the most important one to read. It discusses the Five Pillars of a Well Architected Framework, with each pillar having a whitepaper of its own, and can all be found on this webpage. Be sure to understand well architected framework not just conceptually, but also in actual practice and application.\nFramework Overview\nOperational Excellence Pillar\nSecurity Pillar\nReliability Pillar\nPerformance Efficiency Pillar\nCost Optimization Pillar\n\nAWS Well-Architected Lenses\nLenses extend the guidance offered by AWS Well-Architected to specific industry and technology domains.\nServerless Applications Lens – AWS Well-Architected\n\nAdditional SAA-C02 Whitepapers\nAWS Security Practices: This paper supplements your study on the AWS services and features such as IAM, Security Groups, NACLs, etc. You should read this paper since security specific questions occasionally pop up in the exam.\nAWS Storage Services Overview:  This paper supplements your study on the different AWS Storage options such as S3, EBS, EFS, Glacier, etc. It contains a good detail of information and comparison for each storage service, which is crucial in knowing the best service to use for a situation.\nBuilding Fault-Tolerant Applications on AWS: This paper discusses the many ways you can ensure your applications are fault-tolerant in AWS. It also contains multiple scenarios where the practices are applied and which AWS services were crucial for the scenario.\n\nFAQs\nFocus on the following FAQs\nEC2\nS3\nVPC\nRoute 53\nRDS\nSQS\n\nTake a look a these FAQs\nIAM\nELB\nLambda\nEBS\nEFS\nAurora\nDynamodb\nElasticache\nCloudfront\nAPI gateway\nAutoscaling\nSNS","wordCount":386,"type":"Content"},{"name":"Security and Identity","description":"IAM, Resource Access Manager, Cognito, Secrets Manager, GuardDuty, Macie, KMS, Cloud HSM, Directory Service, WAF, Shield","topics":["security"],"path":"/securityandidentity","text":"","wordCount":0,"type":"TOC"},{"name":"IAM","description":"Manage access to AWS resources","topics":["security","iam"],"path":"/securityandidentity/iam","text":"AWS Identity and Access Management (IAM) enables you to securely control access to AWS services and resources for your users.\n\nAllows for centralized control and shared access to your AWS Account and/or AWS services\nBy default when you create a user, they have NO permissions to do anything\nRoot account has full admin access upon account creation\nNot region specific, can be shared between all regions\nGranular permission sets for AWS resources\nIncludes Federation Integration which taps into Active Directory, Facebook, Linkedin, etc. for authentication (Identity Federation)\nMulti-factor authentication support\nAllows configuration of temporary access for users, devices and services\nSet up and manage password policy and password rotation policy for IAM users\nIntegration with many different AWS services\nSupports PCI DSS compliance (Payment Card Industry Data Security Standard)\nAccess can be applied to\n  Users - End users (people)\n  Groups - Collection of users under one set of permissions\n  Roles - Assigned to AWS resources. Example: specifying what the resource (such as EC2) is allowed to access on another resource (S3)\n  Policies - JSON Document that defines one or more permissions\nPolicies can be applied to users, groups and roles\nYou can assign up to 10 managed policies to a single group\nPolicy documents must have a version, and a statement in the body; The statement must consist of:\n  Effects (Allow, Deny),\n  Actions (Which action to allow/deny such a * for all actions), and\n  Resources (affected resources such as * for all resources)\nAll resources can share the same policy document\nThere are 3 different types of roles\n  Service Roles\n  Cross account access roles: Used when you have multiple AWS accounts and another AWS account must interact with the current AWS account.\n  Identity provider access roles: Roles for Facebook, Google or similar Identity providers.\nIn order for a new IAM user to be able to log into the console, the user must have a password set.\nBy default a new users access is only accomplished through the use of the access key/secret key\nIf the users password is a generated password, it also will only be shown at the time of creation.\nCustomizable Console Sign-in link can be configured on the main IAM page (ie aws.yourdomain.com)\nCustomizable Console Sign-in links must be globally unique. If a sign in link name is already taken, you must choose an alternative\nRoot account is email address that you used to register your account\nRecommended that root account:\n  should not be used for login\n  should be secured with Multi-factor Authentication (MFA)\nCan create Access Key ID & Secret Access Keys to allow IAM users (or service accounts) to be used with AWS CLI or API calls.\nAccess Key ID & Secret Access Keys are not the same as Username & Password and are not interchangeable.\n  Username & Password are used to login to console.\n  Access Key ID & Secret Access Keys are used for programmatic access (via the APIs or CLI)\nWhen creating a user's credentials, you can only see/download the credentials at the time of creation not after.\nAccess Keys can be retired, and new ones can be created in the event that secret access keys are lost.\nTo create a user password, once the users have been created, choose the user you want to set the password for and from the User Actions drop list, click manage password. Here you can opt to create a generated or custom password. If generated, there is an option to force the user to set a custom password on next login. Once a generated password has been issued, you can see the password which is the same as the access keys. Its shown once only.\nClick on Policies from the left side menu and choose the policies that you want to apply to your users. When you pick a policy that you want applied to a user, select the policy, and then from the top Policy Actions drop menu, choose attach and select the user that you want to assign the policy to.\n\nIAM ARNs\narn:partition:service:region:account:resource\n\nWhere:\npartition identifies the partition that the resource is in. For standard AWS Regions, the partition is aws. If you have resources in other partitions, the partition is aws-partitionname. For example, the partition for resources in the China (Beijing) Region is aws-cn. You cannot delegate access between accounts in different partitions.\nservice identifies the AWS product. For IAM resources, this is always iam.\nregion is the Region the resource resides in. For IAM resources, this is always kept blank.\naccount is the AWS account ID with no hyphens (for example, 123456789012).\nresource is the portion that identifies the specific resource by name.\n\n| Resource or Operation                                  | Default Limit |\n| ------------------------------------------------------ | ------------- |\n| Groups per account                                     | 100           |\n| Instance profiles                                      | 100           |\n| Roles                                                  | 250           |\n| Server Certificates                                    | 20            |\n| Users                                                  | 5000          |\n| Number of policies allowed to attach to a single group | 10            |\n\nCreate a group:\nimages/iam/iam1.png\n\nPolicy: json defined permissions\nimages/iam/iam2.png\n\nIAM roles are a secure way to grant permissions to entities that you trust.\nimages/iam/iam3.png","wordCount":799,"type":"Content"},{"name":"RAM","description":"Share AWS resources with other AWS accounts.","topics":["security","RAM"],"path":"/securityandidentity/ram","text":"AWS Resource Access Manager (RAM) is a service that enables you to easily and securely share AWS resources with any AWS account or within your AWS Organization.\nYou can share the following resources with RAM:\n  AWS Transit Gateways,\n  Subnets,\n  AWS License Manager configurations,\n  Amazon Route 53 Resolver rules.\n\nMany organizations use multiple accounts to create administrative or billing isolation, and to limit the impact of errors. RAM eliminates the need to create duplicate resources in multiple accounts, reducing the operational overhead of managing those resources in every single account you own. You can create resources centrally in a multi-account environment, and use RAM to share those resources across accounts in 3 simple steps:\n\n1- create a Resource Share,\n2- specify resources,\n3- specify principals with whom to share. A principal can be:\n  AWS accounts,\n  organizational units (OU),\n  or an entire organization from AWS Organizations\n\nRAM is available to you at no additional charge (free).\n\nBenefits\nReduce Operational Overhead\nProcure AWS resources centrally, and use RAM to share resources such as subnets or License Manager configurations with other accounts. This eliminates the need to provision duplicate resources in every account in a multi-account environment, reducing the operational overhead of managing those resources in every account.\nImprove Security and Visibility\nRAM leverages existing policies and permissions set in AWS Identity and Access Management (IAM) to govern the consumption of shared resources. RAM also provides comprehensive visibility into shared resources to set alarms and visualize logs through integration with Amazon CloudWatch and AWS CloudTrail.\n\nOptimize Costs\nSharing resources such as AWS License Manager configurations across accounts allows you to leverage licenses in multiple parts of your company to increase utilization and optimize costs.\n\nHow it works?\nimages/ram/ram1.png","wordCount":268,"type":"Content"},{"name":"Cognito","description":"Authentication & Authorization for Web and mobile apps","topics":["security","Cognito","Web Identity Federation"],"path":"/securityandidentity/cognito","text":"Amazon Cognito lets you easily add user sign-up and sign-in to your mobile and web apps.\nAmazon Cognito has two authentication methods, independent of one another\n  Sign in via third party federation\n  Cognito user pools\nyou have the options to authenticate users through social identity providers such as Facebook, Twitter, or Amazon, via SAML 2.0 identity solutions, or by using your own identity system.\nCan enable users to sign up or sign in with an email address or phone number.\nsupport SMS-based multi-factor authentication (MFA).\n\nUser Pools & Identity Pools\nUser Pools\nfor authentication purpose\nmanage sign-up sign-in functionality for mobile and web app\ncan sign-in directly (use email, password) or use Facebook, Amazon, Google\n\nIdentity Pools\nfor authorization purpose\nprovide temporary AWS credentials to access AWS services like S3 or DynamoDB\n\nimages/cognito/cognito1.png\n\nCognito Synchronisation\nAmazon Cognito enables you to save data locally on users devices, allowing your applications to work even when the devices are offline.\nYou can thencsynchronize data across user devices so that their app experience remains consistent regardless of the device they use.\nSave mobile data like game states or preferences.\nUse SNS to send notification to all devices associated with user identity whenever data stored in the cloud changes (email address change, password, setting in app... )\nimages/cognito/cognito2.png\n","wordCount":193,"type":"Content"},{"name":"Secrets Manager","description":"Easily rotate, manage, and retrieve secrets throughout their lifecycle","topics":["security","Secrets Manager"],"path":"/securityandidentity/secretsmanager","text":"Protects secrets needed to access your applications, services, and IT resources.\nEnables you to easily rotate, manage, and retrieve:\n  database credentials,\n  API keys,\n  and other secrets throughout their lifecycle.\nUsers and applications retrieve secrets with a call to Secrets Manager APIs, eliminating the need to hardcode sensitive information in plain text.\nSecrets Manager offers secret rotation with built-in integration for:\n  Amazon RDS,\n  Amazon Redshift,\n  and Amazon DocumentDB.\nthe service is extensible to other types of secrets, including API keys and OAuth tokens.\nEnables you to control access to secrets using fine-grained permissions and audit secret rotation centrally for resources in the AWS Cloud, third-party services, and on-premises.\n\nHow it works?\nimages/secretsmanager/secretsmanager1.png","wordCount":103,"type":"Content"},{"name":"GuardDuty","description":"Intelligent threat detection to protect your AWS accounts and workloads","topics":["security","GuardDuty"],"path":"/securityandidentity/guardduty","text":"Amazon GuardDuty is a continuous security monitoring service that analyzes and processes the following Data sources:\n  VPC Flow Logs,\n  CloudTrail management event logs,\n  Cloudtrail S3 data event logs,\n  DNS logs.\nIt uses threat intelligence feeds, such as lists of malicious IP addresses and domains, and machine learning to identify unexpected and potentially unauthorized and malicious activity within your AWS environment.\nThis can include issues like:\n  escalations of privileges,\n  uses of exposed credentials,\n  communication with malicious IP addresses, URLs, or domains.\nGuardDuty can detect compromised EC2 instances serving malware or mining bitcoin.\nIt also monitors AWS account access behavior for signs of compromise, such as unauthorized infrastructure deployments, like instances deployed in a Region that has never been used, or unusual API calls, like a password policy change to reduce password strength.\nGuardDuty informs you of the status of your AWS environment by producing security findings that you can view:\n  in the GuardDuty console\n  or through Amazon CloudWatch events.","wordCount":152,"type":"Content"},{"name":"Macie","description":"Discover and protect your sensitive data at scale","topics":["security","Macie"],"path":"/securityandidentity/macie","text":"Macie automatically provides an inventory of Amazon S3 buckets including a list of:\n  unencrypted buckets,\n  publicly accessible buckets,\n  buckets shared with AWS accounts outside those you have defined in AWS Organizations.\nThen, Macie applies machine learning and pattern matching techniques to the buckets you select to identify and alert you to sensitive data, such as personally identifiable information (PII).\nMacie’s alerts, or findings, can be searched and filtered in the AWS Management Console and sent to Amazon CloudWatch Events for easy integration with existing workflow or event management systems, or to be used in combination with AWS services, such as AWS Step Functions to take automated remediation actions.\nHelp you meet regulations, such as the Health Insurance Portability and Accountability Act (HIPAA) and General Data Privacy Regulation (GDPR).\n\nHow it works?\nimages/macie/macie1.png","wordCount":126,"type":"Content"},{"name":"KMS","description":"Create keys and control encryption across AWS and beyond.","topics":["security","Kms"],"path":"/securityandidentity/kms","text":"Overview\nKey management is the management of cryptographic keys within a cryptosystem. This includes dealing with the generation, exchange, storage, use, and replacement of keys.\n\nAWS offers two services that provide you with the ability to manage your own symmetric or asymmetric cryptographic keys:\nAWS KMS: A service enabling you to generate, store, enable/disable, and delete symmetric keys\nAWS CloudHSM: A service providing you with secure cryptographic key storage by making Hardware Security Modules (HSMs) available on the AWS cloud.\n\nFeatures\nManaged service that enables you to easily create and control the keys used for cryptographic operations.\nA shared key management infrastructure with FIPS 140–2 Level 2 compliance\nCurrently supports 50+ AWS services.\nA regional service\nHighly Available (HA). Keys are kept as multiple copies in a single region to make sure its durability and reliability.\nIt integrates fully with IAM and CloudTrail for Permission Management and Auditing respectively.\nSupports both symmetric and asymmetric key encryption. Until 2019, it only supported symmetric but now it supports both key encryption aspects With the support of asymmetric key encryption, it widened the aspect of interoperability by being able to connect to many other Key Management Infrastructures in the market.\nAWS KMS lets you create keys that can never be exported from the service and that can be used to encrypt and decrypt data based on policies you define.\nBy using AWS KMS, you gain more control over access to data you encrypt.\nYou can use the key management and cryptographic features directly in your applications or through AWS cloud services that are integrated with AWS KMS.\nWhether you are writing applications for AWS or using AWS cloud services, AWS KMS enables you to maintain control over who can use your keys and gain access to your encrypted data.\n\nEncryption comparaison\n|           | En/decrypt | Secret key - storage | Secret key - management | Server or Client Encryption |\n| --------- | ---------- | -------------------- | ----------------------- | --------------------------- |\n| SSE - S3  | S3         | S3                   | S3                      | Server                      |\n| SSE - C   | AWS        | Client               | Client                  | Server                      |\n| SSE - KMS | AWS        | AWS                  | AWS/Client              | Server                      |\n| CSE       | Client     | Client               | Client                  | Client                      |","wordCount":355,"type":"Content"},{"name":"Cloud HSM","description":"Protects keys on a on a FIPS 140-2 Level 3 validated hardware","topics":["security","CloudHSM"],"path":"/securityandidentity/cloudhsm","text":"Overview\nThere are multiple cryptography services are offered by AWS.\nAWS Key Management Service (KMS)\nAWS Certificate Manager\nAWS Secrets Manager\nAWS Cloud HSM\n\nTheoretically, all above services are working as Hardware Security Modules (HSMs). Out of them, CloudHSM has given more control to you and you are basically responsible to manage it. All the others basically managed by AWS.\n\nFeatures\nThe Hardware Security Module (HSM) is a dedicated hardware security module under your exclusive control. That means you get the dedicated access to HSM appliances. Only you have access to your keys and operations on the keys.\nHSMs are located in AWS data centers and it is managed and monitored by AWS.\nHSMs are FIPS 140–2 Level 3 complied and handled inside your VPC, isolated from the rest of the network.\n","wordCount":122,"type":"Content"},{"name":"Directory Service","description":"Host and Manage Active Directory.","topics":["security","Directory Service"],"path":"/securityandidentity/directoryservice","text":"AWS Directory Service is a managed service offering that provides directories that contain\ninformation about your organization, including users, groups, computers, and other\nresources.\n\nOverview\nYou can choose from 4 directory types:\nAWS Managed Microsoft AD (Active Directory)\nSimple AD\nAD Connector\nAmazon Cognito User Pools\n\nAWS Managed Microsoft AD\nAWS Managed Microsoft AD is highly-available: each directory is deployed across multiple AZs.\nMonitoring automatically detects and replaces domain controllers that fail.\nIn addition, data replication and automated daily snapshots are configured for you.\nThere is no software to install, and AWS handles all of the patching and software updates.\nYou can set up trust relationships with your existing Active Directory domains to extend those directories to AWS cloud services\n\nSimple AD\nSimple AD is compatible with Microsoft Active Directory (Microsoft AD)\npowered by Linux-Samba Active Directory\nSimple AD supports commonly used ActiveDirectory features such as:\n  user accounts, group memberships,\n  domain-joining Amazon EC2 instances running Linux and Microsoft Windows,\n  Kerberos-based Single Sign-On (SSO),\n  group policies\nSimple AD provides daily automated snapshots to enable point-in-time recovery.\nyou cannot set up trust relationships between Simple AD and other Active Directory domains\n\nAD Connector\nAD Connector is a proxy for redirecting directory requests to your on-premise Microsoft Active Directory without caching any information in the cloud.\nAfter setup, your users can use their existing corporate credentials to log on to AWS applications, such as\n  Amazon WorkSpaces,\n  Amazon WorkDocs,\n  or Amazon WorkMail.\nWith the proper IAM permissions, they can also access the AWS Management Console and manage AWS resources such as Amazon EC2 instances or Amazon S3 buckets.\nWith AD Connector, you continue to manage your Active Directory as usual. For example, adding new users, adding new groups, or updating passwords are all accomplished using standard directory administration tools with your on-premises directory.\n\nAmazon Cognito User Pools\nThis directory type will simply redirect you to Cognito.\n\nWhen to use each type?\nAWS Managed Microsoft AD\nThis Directory Service is your best choice if:\nyou have  >= 5,000 users\nand need a trust relationship set up between an AWS-hosted directory and your on-premises directories.\n\nSimple AD\nIn most cases, Simple AD is the least expensive option and your best choice if\nyou have <= 5,000 users\nand don’t need the more advanced Microsoft Active Directory features.\n\nAD Connector\nAD Connector is your best choice when you want to use your existing on-premises directory with AWS cloud services.","wordCount":363,"type":"Content"},{"name":"WAF","description":"Protect web applications from common web exploits","topics":["security","WAF","Firewall"],"path":"/securityandidentity/waf","text":"AWS Web Application Firewall (WAF) monitors HTTP and HTTPS request that are forwarded to\n  CloudFront\n  ALB (Application Load Balancer)\n  or API Gateway\nWAF is an Application level firewall, sit in layer 7 in OSI model.\nThe resources are protected via rules such as\n  IP addresses,\n  country,\n  request headers,\n  request body,\n  query strings,\n  string from regex,\n  request length ,\n  SQL injection,\n  XSS\n  ..\nAws managed rules or custom rules can be used\n\nWhen a request matches a rule statements, the action can be:\n  allow\n  block\n  or count","wordCount":81,"type":"Content"},{"name":"Shield","description":"Managed DDoS protection service","topics":["security","Shield","DDos"],"path":"/securityandidentity/shield","text":"AWS provides two levels of protection against DDoS attacks:\n  AWS Shield Standard\n  AWS Shield Advanced.\n\nAWS Shield Standard is automatically included at no extra cost beyond what you already pay for AWS WAF and your other AWS services.\nFor added protection against DDoS attacks, AWS offers AWS Shield Advanced.\nAWS Shield Advanced provides expanded DDoS attack protection for\n  Amazon EC2 instances,\n  Elastic Load Balancing (ALB, NLB and CLB),\n  CloudFront,\n  Route 53 hosted zones.","wordCount":69,"type":"Content"},{"name":"Storage","description":"S3, EFS, EBS, FSX, Storage Gateway","topics":["storage"],"path":"/storage","text":"","wordCount":0,"type":"TOC"},{"name":"S3","description":"Store objects in cloud","topics":["storage","S3"],"path":"/storage/s3","text":"Buckets\nData Consistency Model\nStorage Classes\n  Frequently Accessed Objects\n  Infrequently Accessed Objects\n  Intelligent Tiering\n  GLACIER\n  Glacier Deep Archive\n  Comparaison table\nBucket Configurations\nObjects\n  Tagging\n  Object Delete\n  S3 Select\n  Lifecycle Management\nNetworking\n  virtual-hosted style access\n  Path-style access (will be Deprecated)\n  Example\nSecurity\n  Resource Based Policies\n    Bucket Policies\n    ACL (Access Control Lists)\n    User Policies\nVersioning\nEncryption\nS3 Events Notification\nCross Region Replication (CRR)\n  When to use\n  Requirements\n\nBuckets\nFor each bucket, you can:\n  Control access to it (create, delete, and list objects in the bucket)\n  View access logs for it and its objects\n  Choose the geographical region where to store the bucket and its contents.\n\nBucket name must be a unique DNS-compliant name.\n  The name must be unique across all existing bucket names in Amazon S3.\n  After you create the bucket you cannot change the name.\n  The bucket name is visible in the URL that points to the objects that you’re going to put in your bucket.\n\nBy default, you can create up to 100 buckets in each of your AWS accounts.\nYou can’t change its Region after creation.\nYou can host static websites by configuring your bucket for website hosting.\nYou can’t delete an S3 bucket using the Amazon S3 console if the bucket contains 100,000 or more objects.\nYou can’t delete an S3 bucket using the AWS CLI if versioning is enabled.\n\nData Consistency Model\nread-after-write consistency for PUTS of new objects in your S3 bucket in all regions\neventual consistency for overwrite PUTS and DELETES in all regions\n\nStorage Classes\nFor S3 Standard, S3 Standard-IA and Glacier storage classes, your objects are automatically stored across multiple devices spanning a minimum of 3 Availability Zones.\n\nFrequently Accessed Objects\n  S3 STANDARD for general-purpose storage of frequently accessed data.\n\nInfrequently Accessed Objects\n  S3 STANDARD_IA for long-lived, but less frequently accessed data. It stores the object data redundantly across multiple AZs.\n  S3 ONEZONEIA stores the object data in only one AZ. Less expensive than STANDARDIA, but data is not resilient to the physical loss of the AZ.\n  These two storage classes are suitable for objects\n    larger than 128 KB that you plan to store for at least 30 days.\n    If an object is less than 128 KB, Amazon S3 charges you for 128 KB.\n    If you delete an object before the 30-day minimum, you are charged for 30 days\n\nIntelligent Tiering\n  designed for customers who want to optimize storage costs automatically when data access patterns change, without performance impact or operational overhead.\n  delivers automatic cost savings by moving data between two access tiers (frequent access and infrequent access) when access patterns change/\n  ideal for data with unknown or changing access patterns.\n  S3 Intelligent-Tiering monitors access patterns and moves objects that have not been accessed for 30 consecutive days to the infrequent access tier. If an object in the infrequent access tier is accessed later, it is automatically moved back to the frequent access tier.\n  There are no retrieval fees in S3 Intelligent-Tiering.\n\nGLACIER\nFor long-term archive\nArchived objects are not available for real-time access. You must first restore the objects before you can access them.\nYou cannot specify GLACIER as the storage class at the time that you create an object.\nGlacier objects are visible through S3 only.\nRetrieval options:\n  Expedited\n    allows you to quickly access your data when occasional urgent requests for a subset of archives are required.\n    For all but the largest archived objects, data accessed are typically made available within 1–5 minutes.\n  Standard\n    allows you to access any of your archived objects within several hours.\n    Standard retrievals typically complete within 3–5 hours.\n    This is the default option for retrieval requests that do not specify the retrieval option.\n  Bulk\n    Glacier’s lowest-cost retrieval option, enabling you to retrieve large amounts, even petabytes, of data inexpensively in a day.\n    Bulk retrievals typically complete within 5–12 hours.\n\nGlacier Deep Archive\nA new Amazon S3 storage class providing secure and durable object storage for long-term retention of data that is accessed rarely in a year.\nS3 Glacier Deep Archive offers the lowest cost storage in the cloud, at prices lower than storing and maintaining data in on-premises magnetic tape libraries or archiving data offsite.\nAll objects stored in the S3 Glacier Deep Archive storage class are replicated and stored across at least 3 geographically-dispersed AZs,\nprotected by 99.999999999% durability,\ncan be restored within 12 hours or less.\nS3 Glacier Deep Archive also offers a* bulk retrieval* option, where you can retrieve petabytes of data within 48 hours.\n\nComparaison table\n\n|                                    | S3 Standard            | S3 intelligent Tiering | S3 Standard IA         | S3 One zone-IA         | S3 Glacier              | S3 Glacier Deep Archive |\n| ---------------------------------- | ---------------------- | ---------------------- | ---------------------- | ---------------------- | ----------------------- | ----------------------- |\n| Designed for durability          | 99.999999999% (11 9's) | 99.999999999% (11 9's) | 99.999999999% (11 9's) | 99.999999999% (11 9's) | 99.999999999% (11 9's)  | 99.999999999% (11 9's)  |\n| Designed for availability          | 99.99%                 | 99.9%                  | 99.9%                  | 99.5%                  | 99.99%                  | 99.99%                  |\n| Availability SLA                   | 99.9%                  | 99%                    | 99%                    | 99%                    | 99.9%                   | 99.9%                   |\n| Availability Zones                 | >=3                    | >=3                    | >=3                    | 1                      | >=3                     | >=3                     |\n| Minimum capacity charge per object | N/A                    | N/A                    | 128KB                  | 128KB                  | 40KB                    | 40KB                    |\n| Minimum storage duration charge    | N/A                    | 30 days                | 30 days                | 30 days                | 90 days                 | 180 days                |\n| Retrieval fees                     | N/A                    | N/A                    | per GB retrieved       | per GB retrieved       | per GB retrieved        | per GB retrieved        |\n| First byte latency                 | ms                     | ms                     | ms                     | ms                     | select minutes ou hours | select hours            |\n| Storage type                       | Object                 | Object                 | Object                 | Object                 | Object                  | Object                  |\n| Lifecycle transitions              | yes                    | yes                    | yes                    | yes                    | yes                     | yes                     |\n\nBucket Configurations\n| Configuration | Description |\n|-|-|\n| Location | Specify the AWS Region where you want S3 to create the bucket. |\n| Policy and ACL (access control list) | All your resources are private by default. Use bucket policy and ACL options to grant and manage bucket-level permissions. |\n| CORS (cross-origin resource sharing) | You can configure your bucket to allow cross-origin requests. |\n| Website | You can configure your bucket for static website hosting. |\n| Logging | Logging enables you to track requests for access to your bucket. Each access log record provides details about a single access request, such as the requester, bucket name, request time, request action, response status, and error code, if any. |\n| Event notification | You can enable your bucket to send you notifications of specified bucket events. |\n| Versioning | AWS recommends VERSIONING AS A BEST PRACTICE to recover objects from being deleted or overwritten by mistake. |\n| Lifecycle | You can define lifecycle rules for objects in your bucket that have a well-defined lifecycle. |\n| Cross-region Replication | Cross-region replication is the automatic, asynchronous copying of objects across buckets in different AWS Regions. |\n| Tagging | S3 provides the tagging subresource to store and manage tags on a bucket. AWS generates a cost allocation report with usage and costs aggregated by your tags. |\n| RequestPayment | By default, the AWS account that creates the bucket (the bucket owner) pays for downloads from the bucket. The bucket owner can specify that the person requesting the download will be charged for the download. |\n| Transfer Acceleration | Transfer Acceleration enables fast, easy, and secure transfers of files over long distances between your client and an S3 bucket. It takes advantage of Amazon CloudFront's globally distributed edge locations. |\n\nObjects\nAre private by default. Grant permissions to other users.\nEach S3 object has :\n  data,\n  a key,\n  and metadata.\nYou cannot modify object metadata after object is uploaded.\nTwo kinds of metadata\n  System metadata\n  User-defined metadata\n    key-value pair that you provide.\n      must start with x-amz-meta.\nS3 can store objects of size 0 bytes to 5 TB\nYou can upload and copy objects of up to 5 GB in size in a single operation.\nFor objects greater than 5 GB up to 5 TB, you must use the multipart upload API.\nTagging\nYou can associate up to 10 tags with an object.\nTags associated with an object must have unique tag keys.\ntag key can be up to 128 Unicode characters in length and\ntag values can be up to 256 Unicode characters in length.\nKey and values are case sensitive.\nObject Delete\nDeleting Objects from a Version-Enabled Bucket\n  Specify a non-versioned delete request – specify only the object’s key, and not the version ID.\n  Specify a versioned delete request – specify both the key and also a version ID.\nDeleting Objects from an MFA-Enabled Bucket\n  If you provide an invalid MFA token, the request always fails.\n  If you are not deleting a versioned object, and you don’t provide an MFA token, the delete succeeds.\nS3 Select\nS3 Select is an Amazon S3 capability designed to pull out only the data you need from an object, which can dramatically improve the performance and reduce the cost of applications that need to access data in S3.\nWorks on objects stored in the following formats:\n  CSV\n  JSON\n  Apache Parquet\n  BZIP2 compression for CSV and JSON objects.\nCloudWatch Metrics for S3 Select lets you monitor S3 Select usage for your applications.\n\nLifecycle Management\nA lifecycle configuration is a set of rules that define actions that is applied to a group of objects.\nimages/s3/s3-1.png\n\nTransition actions\n  Define when objects transition to another storage class.\n  For S3-IA and S3-One-Zone, the objects must be stored at least 30 days in the current storage class before you can transition them to another class.\n  images/s3/s3-2.gif\nExpiration actions Define when objects expire. S3 deletes expired objects on your behalf.\n\nNetworking\nvirtual-hosted style access\nAmazon S3 routes any virtual hosted–style requests to the US East (N. Virginia) region by default if you use the endpoint s3.amazonaws.com, instead of the region-specific endpoint.\n\nFormat:\nhttp://[bucket].s3.amazonaws.com\nhttp://[bucket].s3 . [aws-region].amazonaws.com\nhttp://[bucket].s3 - [aws-region].amazonaws.com\n\nPath-style access (will be Deprecated)\nIn a path-style URL, the endpoint you use must match the Region in which the bucket resides.\n\nFormat:\nUS East (N. Virginia) Region endpoint: http://s3.amazonaws.com/[bucket]\nRegion-specific endpoint:\nhttp://s3 . [aws-region].amazonaws.com/[bucket]\nhttp://s3 - [aws-region].amazonaws.com/[bucket]\n\nExample\nIn this example,\nmy-bucket1 is a bucket names in us-east-2 region\n/images/image1.jpegis an object key.\n\nThese are valid virtual-hosted style urls:\nhttps://my-bucket1.s3.amazonaws.com/images/image1.jpeg\nhttps://my-bucket1.s3.us-east-2.amazonaws.com/images/image1.jpeg\nhttps://my-bucket1.s3-us-east-2.amazonaws.com/images/image1.jpeg\n\nThese are valid path-hosted style urls:\nhttps://s3.us-east-2.amazonaws.com/my-bucket1/images/image1.jpeg\nhttps://s3-us-east-2.amazonaws.com/my-bucket1/images/image1.jpeg\n\nSecurity\nPolicies contain the following:\n  Resources: buckets and objects\n  Actions: set of operations\n  Effect : allow or deny\n  Principal: the account, service or user who is allowed access to the actions and resources in the statement.\n\nResource Based Policies\nBucket Policies\nProvides centralized access control to buckets and objects based on a variety of conditions:\n  S3 operations,\n  requesters,\n  resources,\n  and aspects of the request (e.g., IP address).\nCan either add or deny permissions across all (or a subset) of objects within a bucket.\nIAM users need additional permissions from root account to perform bucket operations.\nBucket policies are limited to 20 KB in size.\n\nACL (Access Control Lists)\nA list of grants identifying grantee and permission granted.\nACLs use an S3–specific XML schema.\nYou can grant permissions only to other AWS accounts, not to users in your account.\nYou cannot grant conditional permissions\nYou cannot explicitly deny permissions.\nObject ACLs are limited to 100 granted permissions per ACL.\nThe only recommended use case for the bucket ACL is to grant write permissions to the S3 Log Delivery group.\n\nimages/s3/s3-3.png\n\nUser Policies\nsee IAM\nimages/s3/s3-4.png\n\nVersioning\nUse versioning to keep multiple versions of an object in one bucket.\nprotects from unintended overwrites and deletions.\narchive objects so you have access to previous versions.\nversioning is disabled by default.\nWhen you PUT an object in a versioning-enabled bucket, the non-current version is not overwritten.\nimages/s3/s3-5.png\nWhen you DELETE an object, all versions remain in the bucket and Amazon S3 inserts a delete marker.\nimages/s3/s3-6.png\nPerforming a simple GET Object request when the current version is a delete marker returns a 404 Not Found error.\nYou can, however, GET a non-current version of an object by specifying its version ID.\nimages/s3/s3-7.png\nYou can permanently delete an object by specifying the version you want to delete.\nOnly the owner of an Amazon S3 bucket can permanently delete a version.\n\nEncryption\nServer-side Encryption using\n  Amazon S3-Managed Keys (SSE-S3)\n  AWS KMS-Managed Keys (SSE-KMS)\n  Customer-Provided Keys (SSE-C)\nClient-side Encryption using\n  AWS KMS-managed customer master key\n  client-side master key\n\nS3 Events Notification\nTo enable notifications, add a notification configuration identifying:\n  the events to be published\n  the destinations where to send the event notifications.\nCan publish following events:\n  A new object created event\n  An object removed event\n  A Reduced Redundancy Storage (RRS) object lost event\nSupports the following destinations for your events:\n  SNS topic\n  SQS queue\n  AWS Lambda\n\nCross Region Replication (CRR)\nEnables automatic, asynchronous copying of objects across buckets in different AWS Regions.\nWhen to use\nComply with compliance requirements\nMinimize latency\nIncrease operational efficiency\nMaintain object copies under different ownership\n\nRequirements\nBoth source and destination buckets must have versioning enabled.\nThe source and destination buckets must be in different AWS Regions.\nS3 must have permissions to replicate objects from the source bucket to the destination bucket on your behalf.\nthe object owner must grant the bucket owner READ and READ_ACP permissions with the object ACL.\n\nS3 doesn’t replicate the deletion in the destination bucket. This protects data from malicious deletions.","wordCount":2088,"type":"Content"},{"name":"EFS","description":"Managed file storage for EC2","topics":["storage","EFS"],"path":"/storage/efs","text":"Features\nThe service manages all the file storage infrastructure for you, avoiding the complexity of deploying, patching, and maintaining complex file system configurations.\nEFS supports the NFS V4 (Network File System version 4) protocol.\nMultiple Amazon EC2 instances can access an EFS file system at the same time, providing a common data source for workloads and applications running on more than one instance or server.\nEFS store data and metadata across multiple AZs in the same AWS Region.\nEFS can grow to petabyte scale, drive high levels of throughput, and allow massively parallel access from EC2 instances to your data.\nEFS provides file system access semantics, such as strong data consistency and file locking.\nEFS enables you to control access to your file systems through Portable Operating System Interface (POSIX) permissions.\nMoving your EFS file data can be managed simply with AWS DataSync.\nYou can schedule automatic incremental backups of your EFS file system using the EFS-to-EFS Backup solution.\nAmazon EFS Infrequent Access (EFS IA) is a new storage class for Amazon EFS that is cost-optimized for files that are accessed less frequently.\nCustomers can use EFS IA by creating a new file system and enabling Lifecycle Management. With Lifecycle Management enabled, EFS automatically will move files that have not been accessed for 30 days from the Standard storage class to the Infrequent Access storage class.\n\nPerformance Modes\nGeneral purpose performance mode (default): Ideal for latency-sensitive use cases.\nMax I/O mode: Can scale to higher levels of aggregate throughput and operations per second with a tradeoff of slightly higher latencies for file operations.\n\nThroughput Modes\nBursting Throughput mode (default): Throughput scales as your file system grows.\nProvisioned Throughput mode You specify the throughput of your file system independent of the amount of data stored.\n\nEFS vs EBS vs S3\n\n|  | S3 | EBS | EFS |\n|-|-|-|-|\n| Type of storage | Object storage. You can store virtually any kind of data in any format. | Persistent block level storage for EC2 instances. | POSIX-compliant file storage for EC2 instances |\n| Features | Accessible to anyone or any service with the right permissions | Deliver performance for workloads that require the lowest-latency access to data from a single EC2 instance | Has a file system interface, file system access semantics (such as strong consistency and file locking), and concurrently-accessib storage for multiple EC2 instances: |\n| Max Storage Size | Virtually unlimited | 16 TiB for one volume | Unlimited system size |\n| Max File Size | Individual Amazon S3 objects can range in size to a maximum of 5 terabytes. | Equivalent to the maximum size of your volumes | 47.9 TiB for a single file |\n| Performance (Latency) | Low, for mixed request types, and integration with CloudFront | Lowest, consistent; SSD-backed storages include the highest performance Provisioned OPS SSD and General Purpose SSD that balance price and performance. | Low, consistent; use Max I/O mode for higher performance |\n| Performance (Throughput) | Multiple GBs per second; supports multi-part upload | Up to 2 GB per second. HDD-backed volumes include Throughput Optimized HDD for frequently accessed, throughput intensive workloads and Cold HDD for less frequently accessed data. | 10+ GB per second. Bursting Throughput mode scales with the size of the file system. Provisioned Throughput mode offers higher dedicated throughput than burstring throughput. |\n| Durability | Stored redundantly across multiple AZs; has 99.999999999% durability | Stored redundantly in a single AZ | Stored redundantly across multiple AZs |\n| Availability | S3 Standard 99.99% availability S3 Standard-IA 99.9% availability S3 One Zone-IA 99.5% availability. | Has 99.999% availability | No SLA. Runs in multi-AZ |\n| Scalability | Highly scalable | Manually increase/decrease your memory size. Attach and detach additional volumes to and from your EC2 instance to scale. | EFS file systems are elastic, and automatically grow and shrink as you add and remove files. |\n| Data Accessing | One to millions of connections over the web; S3 provides a REST web services interface | Single EC2 instance in a single AZ | One to thousands of EC2 instances or on-premises servers, from multiple AZs, regions, VPCs, and accounts concurrently |\n| Access Control | Uses bucket policies and IAM user policies. Has Block Public Access settings to help manage public access to resources. | IAM Policies, Roles, and Security Groups | Only resources that can access endpoints in your VPC, called a mount target, can access your file system; POSIX-compliant user and group-level permissions |\n| Encryption Methods | Supports SSL endpoints using the HTTPS protocol, Client-Side and Server-Side Encryption (SSE-S3, SSE-C, SSE-KMS) | Encrypts both data-at-rest and data-in-transit through EBS encryption that uses AWS KMS CMKs. | Encrypt data at rest and in transit. Data at rest encryption uses AWS KMS. Data in-transit uses TLS 1.2. |\n| Backup and Restoration | Use versioning or cross-region replication | All EBS volume types offer durable snapshot capabilities. | EFS to EFS replication through third party tools or AWS DataSync |\n| Pricing | Billing prices are based on the location of your bucket. Lower costs equals lower prices. You get cheaper prices the more you use S3 storage. | You pay GB-month of provisioned storage, provisioned -month, GB-month of snapshot data stored in S3 | You pay for the amount of file system storage used per month. When using the Provisioned Throughput mode you pay for the throughput you provision per month. |\n| Use Cases | Web serving and content management, media and entertainment, backups, big data analytics, data lake | Boot volumes, transactional and NoSQL databases, data warehousing & ETL | Web serving and content management, enterprise applications, media and entertainment, home directories, database backups, developer tools, container storage, big data analytics |\n| Service endpoint | Can be accessed within and outside a VPC (via S3 bucket URL) | Accessed within one's VPC | Accessed within one's VPC |","wordCount":957,"type":"Content"},{"name":"EBS","description":"Block storage device for EC2","topics":["storage","EBS"],"path":"/storage/ebs","text":"Features\nTypes of EBS Volumes\n  General Purpose SSD (gp2)\n  Provisioned IOPS SSD (io1)\n  Throughput Optimized HDD (st1)\n  Cold HDD (sc1)\nEncryption\nEBS Snapshots\n\nBlock level storage volumes for use with EC2 instances.\nWell-suited for use as the primary storage for file systems, databases, or for any applications that require fine granular updates and access to raw, unformatted, block-level storage.\nWell-suited to both database-style applications (random reads and writes), and to throughput-intensive applications (long, continuous reads and writes).\nNew EBS volumes receive their maximum performance the moment that they are available and do not require initialization (formerly known as pre-warming). However, storage blocks on volumes that were restored from snapshots must be initialized (pulled down from Amazon S3 and written to the volume) before you can access the block.\nTermination protection is turned off by default and must be manually enabled (keeps the volume/data when the instance is terminated)\nYou can have up to 5,000 EBS volumes by default\nYou can have up to 10,000 snapshots by default\n\nFeatures\nDifferent types of storage options:\n  General Purpose SSD (gp2),\n  Provisioned IOPS SSD (io1),\n  Throughput Optimized HDD (st1),\n  and Cold HDD (sc1) volumes up to 16 TiB in size.\nYou can mount multiple volumes on the same instance\nyou can mount a volume to multiple instances at a time using Amazon EBS Multi-Attach.\nEnable Multi-Attach on EBS Provisioned IOPS io1 volumes to allow a single volume to be concurrently attached to up to sixteen AWS Nitro System-based Amazon EC2 instances within the same AZ.\nYou can create a file system on top of these volumes, or use them in any other way you would use a block device (like a hard drive).\nYou can use encrypted EBS volumes to meet data-at-rest encryption requirements for regulated/audited data and applications.\nYou can create point-in-time snapshots of EBS volumes, which are persisted to Amazon S3.\nSimilar to AMIs. Snapshots can be copied across AWS regions.\nVolumes are created in a specific AZ, and can then be attached to any instances in that same AZ.\nTo make a volume available outside of the AZ, you can create a snapshot and restore that snapshot to a new volume anywhere in that region.\nYou can copy snapshots to other regions and then restore them to new volumes there, making it easier to leverage multiple AWS regions for geographical expansion, data center migration, and disaster recovery.\nPerformance metrics, such as bandwidth, throughput, latency, and average queue length, provided by Amazon CloudWatch, allow you to monitor the performance of your volumes to make sure that you are providing enough performance for your applications without paying for resources you don’t need.\nYou can detach an EBS volume from an instance explicitly or by terminating the instance. However, if the instance is running, you must first unmount the volume from the instance.\nIf an EBS volume is the root device of an instance, you must stop the instance before you can detach the volume.\nYou can use AWS Backup, an automated and centralized backup service, to protect EBS volumes and your other AWS resources.\nAWS Backup is integrated with the following services to give you a fully managed AWS backup solution:\n  Amazon DynamoDB,\n  Amazon EBS,\n  Amazon RDS,\n  Amazon EFS,\n  AWS Storage Gateway.\nWith AWS Backup, you can configure backups for EBS volumes, automate backup scheduling, set retention policies, and monitor backup and restore activity.\n\nTypes of EBS Volumes\nGeneral Purpose SSD (gp2)\nBase performance of 3 IOPS/GiB, with the ability to burst to 3,000 IOPS for extended periods of time.\nSupport up to 16,000 IOPS and 250 MB/s of throughput.\nThe burst duration of a volume is dependent on the size of the volume, the burst IOPS required, and the credit balance when the burst begins. Burst IO duration is computed using the following formula:\n  > Burst duration  = (Credit balance) [(Burst IOPS) – 3 (Volume size in GiB)]\n\nIf your gp2 volume uses all of its I/O credit balance, the maximum IOPS performance of the volume remains at the baseline IOPS performance level and the volume’s maximum throughput is reduced to the baseline IOPS multiplied by the maximum I/O size.\nThroughput for a gp2 volume can be calculated using the following formula, up to the throughput limit of 160 MiB/s:\n  > Throughput in MiB/s = (Volume size in GiB) (IOPS per GiB) × (I/O size in KiB)\n\nProvisioned IOPS SSD (io1)\nDesigned for I/O-intensive workloads, particularly database workloads, which are sensitive to storage performance and consistency.\nAllows you to specify a consistent IOPS rate when you create the volume\n\nThroughput Optimized HDD (st1)\nLow-cost magnetic storage that focuses on throughput rather than IOPS.\nThroughput of up to 500 MiB/s.\nSubject to throughput and throughput-credit caps, the available throughput of an st1 volume is expressed by the following formula:\n    > Throughput = (Volume size)(Credit accumulation rate per TiB)\n\nCold HDD (sc1)\nLow-cost magnetic storage that focuses on throughput rather than IOPS.\nThroughput of up to 250 MiB/s.\n\n| Volume Type | General Purpose SSD (gp2) | Provisioned IOPS SSD (io1) | Throughput Optimized HDD (st1) | Cold HDD (sc1) |\n| -- | -- | -- | -- | -- |\n| Use Cases | - Recommended for most workloads  - System boot volumes  -  Virtual desktops  - Low-latency interactive apps  - Development and test environments | - Critical business applications that require sustained IOPS performance, or more than 16,000 IOPS or 250 MiB/s of throughput per volume  - When attached to Nitro system EC2 instances, peak performance can go up to 64,000 IOPS and 1,000 MB/s of throughput per volume.  - Large database workloads. | - Streaming workloads requiring consistent, fast throughput at a low price  - Big data  - Data Warehouse   - Log processing | - Throughput-oriented storage for large volumes of data that is infrequently accessed  - Scenarios where the lowest storage cost is important |\n| Boot volume | yes | yes | no | no |\n| Volume Size | 1 GiB – 16 TiB | 4 GiB – 16 TiB | 500 GiB – 16 TiB | 500 GiB – 16 TiB |\n| Max. IOPS/Volume | 16,000 | 64,000 | 500 | 250 |\n| Max. Throughput/Volume | 250 MiB/s | 1000 MiB/s | 500 MiB/s | 250 MiB/s |\n| Dominant Performance Attribute | IOPS | IOPS | MiB/s | MiB/s |\n\nEncryption\nData stored at rest on an encrypted volume, disk I/O, and snapshots created from it are all encrypted.\nAlso provides encryption for data in-transit from EC2 to EBS since encryption occurs on the servers that hosts EC2 instances.\nThe following types of data are encrypted:\n  Data at rest inside the volume\n  All data moving between the volume and the instance\n  All snapshots created from the volume\n  All volumes created from those snapshots\nUses AWS Key Management Service (AWS KMS) master keys when creating encrypted volumes and any snapshots created from your encrypted volumes.\nVolumes restored from encrypted snapshots are automatically encrypted.\nEBS encryption is only available on certain instance types.\nThere is no direct way to encrypt an existing unencrypted volume, or to remove encryption from an encrypted volume. However, you can migrate data between encrypted and unencrypted volumes.\nYou can now enable Amazon Elastic Block Store (EBS) Encryption by Default, ensuring that all new EBS volumes created in your account are encrypted.\n\nEBS Snapshots\nBack up the data on your EBS volumes to S3 by taking point-in-time snapshots.\nSnapshots are incremental backups, which means that only the blocks on the device that have changed after your most recent snapshot are saved. This minimizes the time required to create the snapshot and saves on storage costs by not duplicating data.\nWhen you delete a snapshot, only the data unique to that snapshot is removed.\nimages/ebs/ebs1.png\nYou can share a snapshot across AWS accounts by modifying its access permissions.\nYou can make copies of your own snapshots as well as snapshots that have been shared with you.\nA snapshot is constrained to the Region where it was created.\nEBS snapshots broadly support EBS encryption.\nYou can’t delete a snapshot of the root device of an EBS volume used by a registered AMI. You must first deregister the AMI before you can delete the snapshot.\nEach account can have up to 5 concurrent snapshot copy requests to a single destination Region.\nUser-defined tags are not copied from the source snapshot to the new snapshot.\nSnapshots are constrained to the Region in which they were created. To share a snapshot with another Region, copy the snapshot to that Region.\nSnapshots that you intend to share must instead be encrypted with a custom CMK.\n","wordCount":1361,"type":"Content"},{"name":"FSx","description":"Fully managed third-party file systems optimized for a variety of workloads","topics":["storage","FSx"],"path":"/storage/fsx","text":"Amazon FSx is a fully managed third-party file system solution. It uses SSD storage to provide fast performance with low latency.\nThere are two available FSx solutions available in AWS:\n  Amazon FSx for Windows File Server\n  Amazon FSx for Lustre\nAmazon FSx for Windows File Server\nA fully managed native Microsoft Windows file system with full support for the SMB protocol, Windows NTFS, and Microsoft Active Directory (AD) integration.\nCommon Use Cases\n  File systems that is accessible by multiple users, and can establish permissions at the file or folder level.\n  Application workloads that require shared file storage provided by Windows-based file systems (NTFS) and that use the SMB protocol.\n  Media workflows like media transcoding, processing, and streaming.\n  Data-intensive analytics workloads.\n  Content management and web serving applications such as IIS\nWorks with the following compute services:\n  Amazon EC2\n  Amazon Workspaces instances\n  Amazon AppStream 2.0 instances\n  VMs running in VMWare Cloud on AWS Environments\nWorks with Microsoft Active Directory (AD) to integrate your file system with your existing Windows environments.\nSupports the use of AWS Direct Connect or AWS VPN to access your file systems from your on-premises compute instances.\nMicrosoft Windows File Share\n\nAmazon FSx for Lustre\nA high-performance file system optimized for fast processing of workloads. Lustre is a popular open-source parallel file system.\nSince this is a high-performance parallel file system, you can use Amazon FSx as “hot” storage for your highly accessed files, and Amazon S3 as “cold” storage for rarely accessed files.\nWhen linked to an S3 bucket, an FSx for Lustre file system transparently presents S3 objects as files and allows you to write results back to S3.\nThe Lustre file system provides a POSIX-compliant file system interface.\nAlso supports concurrent access to the same file or directory from thousands of compute instances.\n\nFSx Windows File Servers vs EFS vs FSx for Lustre\n| FSx Windows File Servers|EFS| FSx for Lustre|\n| -- | -- | -- |\n| Windows-based applications, Amazon FSx provides fully managed Windows file servers with features and performance optimized for \"lift-and-shift\" business-critical application workloads including home directories shares), media workflows, and ERP applications. It is accessible from Windows and instances via the SMB protocol. | If you have Linux-based applications, Amazon EFS is a cloud-native fully managed file system that provides simple, scalable, elastic file storage accessible from Linux instances via the NFS protocol. | For compute-intensive and fast processing workloads,like high performance computing (HPC), machine learning. EDA. and media processing, Amazon FSx for Lustre, provides a file system that's optimized for performance, with input and output stored on Amazon S3. |","wordCount":412,"type":"Content"},{"name":"Storage Gateway","description":"Hybrid storage integration","topics":["storage","Storage Gateway"],"path":"/storage/storagegateway","text":"The service enables hybrid storage between on-premises environments and the AWS Cloud.\nIt integrates on-premises enterprise applications and workflows with Amazon’s block and object cloud storage services through industry standard storage protocols.\nThe service stores files as native S3 objects, archives virtual tapes in Amazon Glacier, and stores EBS Snapshots generated by the Volume Gateway with Amazon EBS.\n\nStorage Solutions\nFile Gateway – supports a file interface into S3 and combines a service and a virtual software appliance.\nVolume Gateway – provides cloud-backed storage volumes that you can mount as iSCSI devices from your on-premises application servers.\n  Cached volumes – you store your data in S3 and retain a copy of frequently accessed data subsets locally.\n  images/storagegateway/storagegateway1.png\n  Stored volumes – if you need low-latency access to your entire dataset, first configure your on-premises gateway to store all your data locally. Then asynchronously back up point-in-time snapshots of this data to S3.\n  images/storagegateway/storagegateway2.png\nTape Gateway – archive backup data in Amazon Glacier.","wordCount":154,"type":"Content"},{"name":"Database","description":"RDS, DynamoDB, Redshift, Elasticache","topics":["database"],"path":"/database","text":"","wordCount":0,"type":"TOC"},{"name":"RDS","description":"Managed Relational Database Service","topics":["database","RDS"],"path":"/database/rds","text":"","wordCount":0,"type":"Content"},{"name":"DynamoDB","description":"Managed NoSQL database","topics":["database","DynamoDB"],"path":"/database/dynamodb","text":"","wordCount":0,"type":"Content"},{"name":"ElastiCache","description":"In-Memory Cache","topics":["database","ElastiCache"],"path":"/database/elasticache","text":"","wordCount":0,"type":"Content"},{"name":"Compute","description":"EC2, Lambda, Elastic Beanstalk, ECS","topics":["compute"],"path":"/compute","text":"","wordCount":0,"type":"TOC"},{"name":"EC2","description":"Virtual servers in Cloud","topics":["Compute","EC2"],"path":"/compute/ec2","text":"","wordCount":0,"type":"Content"},{"name":"Lambda","description":"Run code without thinking about servers","topics":["Compute","Lambda"],"path":"/compute/lambda","text":"","wordCount":0,"type":"Content"},{"name":"Elastic Beanstalk","description":"Run and manage Web Apps","topics":["Compute","Elastic Beanstalk"],"path":"/compute/elasticbeanstalk","text":"","wordCount":0,"type":"Content"},{"name":"ECS","description":"Highly secure, reliable, and scalable way to run Docker containers","topics":["Compute","ECS"],"path":"/compute/ecs","text":"","wordCount":0,"type":"Content"},{"name":"Migration & Transfer","description":"DMS, DataSync, Snow","topics":["migration","transfert"],"path":"/migrationandtransfer","text":"","wordCount":0,"type":"TOC"},{"name":"DMS","description":"Managed Database Migration Service","topics":["migration","transfert","DMS"],"path":"/migrationandtransfer/dms","text":"","wordCount":0,"type":"Content"},{"name":"DataSync","description":"simplifies, automates, and accelerates moving data","topics":["migration","transfert","DataSync"],"path":"/migrationandtransfer/datasync","text":"","wordCount":0,"type":"Content"},{"name":"Snow","description":"Large Scale Data Transport","topics":["migration","transfert","Snow"],"path":"/migrationandtransfer/snow","text":"","wordCount":0,"type":"Content"},{"name":"Networking & Content Delivery","description":"VPC, CloudFront, Route 53, API Gateway, Direct Connect, Global Accelerator","topics":["networking","content delivery"],"path":"/networkingandcontentdelivery","text":"","wordCount":0,"type":"TOC"},{"name":"VPC","description":"Isolated Cloud Resources","topics":["networking","content delivery","VPC"],"path":"/networkingandcontentdelivery/vpc","text":"","wordCount":0,"type":"Content"},{"name":"CloudFront","description":"Global Content Delivery Network","topics":["networking","content delivery","CloudFront"],"path":"/networkingandcontentdelivery/cloudfront","text":"","wordCount":0,"type":"Content"},{"name":"Route 53","description":"Scalable DNS and Domain Name Registration","topics":["networking","content delivery","Route 53"],"path":"/networkingandcontentdelivery/route53","text":"","wordCount":0,"type":"Content"},{"name":"API Gateway","description":"Scalable DNS and Domain Name Registration","topics":["networking","content delivery","API Gateway"],"path":"/networkingandcontentdelivery/apigateway","text":"","wordCount":0,"type":"Content"},{"name":"Direct Connect","description":"Dedicated Network Connection to AWS","topics":["networking","content delivery","Direct Connect"],"path":"/networkingandcontentdelivery/directconnect","text":"","wordCount":0,"type":"Content"},{"name":"Global Accelerator","description":"Improve your application’s availability and performance using the AWS Global Network","topics":["networking","content delivery","Global Accelerator"],"path":"/networkingandcontentdelivery/globalaccelerator","text":"","wordCount":0,"type":"Content"},{"name":"Management & Governance","description":"AWS Organizations, Cloud watch, Cloud trail, AWS Auto Scaling, Parameter Store, CloudFormation, AWS Well-Architected Tool","topics":["management","gouvernance"],"path":"/managementandgovernance","text":"","wordCount":0,"type":"TOC"},{"name":"AWS Organizations","description":"Central governance and management across AWS accounts.","topics":["management","gouvernance","AWS Organizations"],"path":"/managementandgovernance/organizations","text":"","wordCount":0,"type":"Content"},{"name":"CloudWatch","description":"Monitor Resources and Applications.","topics":["management","gouvernance","CloudWatch"],"path":"/managementandgovernance/cloudwatch","text":"","wordCount":0,"type":"Content"},{"name":"CloudTrail","description":"Track User Activity and API Usage.","topics":["management","gouvernance","CloudTrail"],"path":"/managementandgovernance/cloudtrail","text":"","wordCount":0,"type":"Content"},{"name":"OpsWorks","description":"Configuration Management with Chef and Puppet.","topics":["management","gouvernance","OpsWorks"],"path":"/managementandgovernance/opsworks","text":"","wordCount":0,"type":"Content"},{"name":"AWS Auto Scaling","description":"Quickly scale your entire application on AWS.","topics":["management","gouvernance","Auto Scaling"],"path":"/managementandgovernance/autoscaling","text":"","wordCount":0,"type":"Content"},{"name":"Systems Manager - Parameter Store","description":"Secrets and configuration data management.","topics":["management","gouvernance","Parameter Store"],"path":"/managementandgovernance/parameterstore","text":"","wordCount":0,"type":"Content"},{"name":"CloudFormation","description":"Infrastructure provisionning as Code.","topics":["management","gouvernance","CloudFormation"],"path":"/managementandgovernance/cloudformation","text":"","wordCount":0,"type":"Content"},{"name":"AWS Well-Architected Tool","description":"Learn best practices, measure, and improve your workloads.","topics":["management","gouvernance","Well-Architected Tool"],"path":"/managementandgovernance/wellarchitectedtool","text":"How it works\nimages/wellarchitectedtool/wellarchitectedtool1.png","wordCount":3,"type":"Content"},{"name":"Developer Tools","description":"X-Ray","topics":["developertools"],"path":"/developertools","text":"","wordCount":0,"type":"TOC"},{"name":"X-Ray","description":"Analyze and Debug Your Applications.","topics":["developertools","X-Ray"],"path":"/developertools/xray","text":"","wordCount":0,"type":"Content"},{"name":"Analytics","description":"Athena, Amazon Redshift, EMR, Kinesis, Data Pipeline","topics":["analytics"],"path":"/analytics","text":"","wordCount":0,"type":"TOC"},{"name":"Athena","description":"Query Data in S3 using SQL.","topics":["analytics","Athena"],"path":"/analytics/athena","text":"","wordCount":0,"type":"Content"},{"name":"Redshift","description":"Data Warehousing","topics":["analytics","Redshift"],"path":"/analytics/redshift","text":"","wordCount":0,"type":"Content"},{"name":"EMR","description":"Managed Hadoop Framework","topics":["analytics","EMR"],"path":"/analytics/emr","text":"","wordCount":0,"type":"Content"},{"name":"Kinesis","description":"Managed Hadoop Framework","topics":["analytics","Kinesis"],"path":"/analytics/kinesis","text":"","wordCount":0,"type":"Content"},{"name":"Data Pipeline","description":"Orchestration for Data-Driven Workflows","topics":["analytics","Data Pipeline"],"path":"/analytics/datapipeline","text":"","wordCount":0,"type":"Content"},{"name":"Application Integration","description":"SNS, SQS, SWF, Step Functions","topics":["application","integration"],"path":"/applicationintegration","text":"","wordCount":0,"type":"TOC"},{"name":"SNS","description":"Managed message topics for Pub/Sub","topics":["application","integration","SNS"],"path":"/applicationintegration/sns","text":"","wordCount":0,"type":"Content"},{"name":"SQS","description":"Managed Message Queues","topics":["application","integration","SQS"],"path":"/applicationintegration/sqs","text":"","wordCount":0,"type":"Content"},{"name":"SWF","description":"Workflow Service for Coordinating Application Components","topics":["application","integration","SWF"],"path":"/applicationintegration/swf","text":"","wordCount":0,"type":"Content"},{"name":"Step Functions","description":"Coordinate Distributed Applications","topics":["application","integration","Step Functions"],"path":"/applicationintegration/stepfunctions","text":"","wordCount":0,"type":"Content"},{"name":"End User Computing","description":"Desktop Workspaces","topics":["End User Computing"],"path":"/endusercomputing","text":"","wordCount":0,"type":"TOC"},{"name":"Desktop Workspaces","description":"Coordinate Distributed Applications","topics":["End User Computing","Desktop Workspaces"],"path":"/endusercomputing/desktopworkspaces","text":"","wordCount":0,"type":"Content"}]